---
title: "Modeling"
author: "Shukry Zablah"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  pdf_document: 
    toc: true
    toc_depth: 4
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Imports

```{r warning=FALSE}
library(dplyr)
library(ROCR)
```

## Load Data

```{r}
train <- readRDS(file = "../data/PIMA_train.Rds")
test <- readRDS(file = "../data/PIMA_test.Rds")
```

## Baseline

```{r}
tally(~ hasDiabetes, data = train)
```

Any model we choose has to have an accuracy higher than 98/(98+196) = 33%. This is the baseline accuracy score.

## Logistic Regression Classifier

### Training

```{r}
clf <- glm(hasDiabetes ~ ., family = binomial(link='logit'), data = train)
summary(clf)
saveRDS(clf, file = "../models/LogisticRegressionClassifier_Full.Rds")
```

### Model Evaluation on Train Set

```{r}
predict <- predict(clf, type = 'response')

with(train, 
     table(hasDiabetes, predict > 0.3))
```

We can see in the confusion matrix that our accuracy is (79 + 147)/(147 + 49 + 19 + 79) = 0.7687. This is with a cutoff of 0.3.

```{r}
ROCRpred <- with(train, 
                 prediction(predict, hasDiabetes))

ROCRperf <- performance(ROCRpred, 'tpr','fpr')

plot(ROCRperf, colorize = TRUE, print.cutoffs.at = seq(0,1,0.1), text.adj = c(-0.2,1.7)); abline(0,1)
```

In the ROC curve we can see that our model is good (the curve is away from the diagonal). Since we care about not predicting a negative result for someone that is actually positive for diabetes (false negative rate), we want to have a larger true positive rate (1 - TPR = FNR). This means that we choose a cutoff near the blue part of the curve, the lower the cutoff the more cautious our model and the less accurate. 

### Model Evaluation on Test Set

```{r}
#only after making decision of cutoff and dealing with missing values....
```